{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retired Material from Edris's Analytics on Trace Data\n",
    "\n",
    "When a particular avenue of testing is unproductive or otherwise abandoned, it is migrated to this workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Verbose\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan, bulk\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax\n",
    "import timeit\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Enable verbose errors, to make debugging easier\n",
    "%xmode Verbose\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Custom Modules\n",
    "\n",
    "As I'm building out functionality, I will first test it directly in this notebook and, as it becomes well-defined, I'll be factoring it out into independent modules. As those modules are added, they'll be included below and I'll be using the autoreload IPython extension so that I don't need to restart my IPython Kernel every time I update a module.\n",
    "\n",
    "| File | Content |\n",
    "| ---- | ------- |\n",
    "| es_queries.py | ElasticSearch queries. |\n",
    "| query_analysis.py | Analysis of ES queries. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import src.es_queries as esq\n",
    "import src.query_analysis as qa\n",
    "import src.make_network as mn\n",
    "import src.draw_network as dn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to ElasticSearch\n",
    "\n",
    "The login method takes a username and password or a filename. If none are provided, the filename defaults to `credentials.key`, but all `.key` files are in `.gitignore` for security reasons. As such, to use that file, it will need to be recreated whenever this repo is cloned and populated with the username on the first line and password on the second line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Successful\n"
     ]
    }
   ],
   "source": [
    "es = esq.es_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Unique Pairs of PerfSonar nodes\n",
    "\n",
    "My initial ElasticSearch queries used `trace_derived` and exclude any pair of PerfSonar nodes that don't have more than $1$ hop and don't have at least $1000$ records.\n",
    "\n",
    "### Attempt 1\n",
    "\n",
    "Aggregate `trace_derived` based on first the sources, then within each source, aggregate on destination. This quickly hit the bucket limit, such that I could only pull $60$ sources and $59$ destinations (when in reality there are over $400$ of each), for a total of $3,540$ pairs (when in reality there are closer to $35,000$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs = esq.get_unique_pairs(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_pairs = []\n",
    "for record in unique_pairs['aggregations']['sources']['buckets']:\n",
    "    for bucket in record['destinations']['buckets']:\n",
    "        ps_pairs.append((record['key'], bucket['key']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3540"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ps_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2\n",
    "\n",
    "I aggregated sources and destinations separately. This allowed me to get all sources and all destinations, but didn't give me information about which were connected.\n",
    "\n",
    "### Attempt 3 (SUCCESS)\n",
    "\n",
    "I removed the query to pull data from `trace_derived` in full. In order to do this, I used `scan` in place of `search`, which returns a generator instead of a dictionary. I could then iterate through this generator to extract the source/destination pairs that meet my criteria and log those that do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_scan = esq.trace_derived_scan(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully identified 24503 perfSONAR pairs.\n"
     ]
    }
   ],
   "source": [
    "qa.reset_vars()\n",
    "qa.make_ps_pairs(td_scan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating over `ps_trace`\n",
    "\n",
    "With the `qa` module now containing a set of valid `ps_pairs`, we can now pull data regarding those pairs from `ps_trace`. As `ps_trace` is huge, we only ever pull a small time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(year=2020, month=7, day=7, hour=8).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "end = datetime(year=2020, month=7, day=7, hour=11).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "ps_trace = esq.ps_trace_scan(es, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Routes\n",
    "### Attempt 1\n",
    "\n",
    "Iterating over the generator, we consider only records that have previously identified perfsonar pairs and keep them only if we are only able to find a single route_sha1 across all records for that pair in the given time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.stable_routes(ps_trace)\n",
    "routes = qa.get_stable_routes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Scan Results\n",
    "\n",
    "Save a day worth of data from `ps_trace` to a parquet file for each of seven days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    start = datetime(year=2020, month=7, day=(7 + i), hour=0).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "    end = datetime(year=2020, month=7, day=(8 + i), hour=0).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "    scan_gen = esq.scan_gen(esq.ps_trace_scan(es, start, end))\n",
    "    filename = 'ps_trace' + str(i) + '.pa'\n",
    "    qa.save_data(scan_gen, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2\n",
    "\n",
    "Iterate over 7 days worth of data. For each route, tracking whether it has changed and how long it lasted in the previous sha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Frequency Dataframe\n",
    "\n",
    "I decided to go with a list, as it was pointless to convert back to a dataframe as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = pd.DataFrame(list(edges.items()), columns=['Edge', 'Count'])\n",
    "edges_df = edges_df.sort_values('Count', ascending=False)\n",
    "edges_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Below this point are disorganized snippets of code used while testing various features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_eval_ps_trace():\n",
    "    start = datetime(year=2020, month=7, day=7, hour=8).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "    end = datetime(year=2020, month=7, day=7, hour=9).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "    ps_trace = esq.ps_trace_scan(es, start, end)\n",
    "    qa.route_changes(ps_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timeit.timeit('try_eval_ps_trace()', setup=\"from __main__ import try_eval_ps_trace\", number=1))\n",
    "# qa.get_ps_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A record of each route and whether \n",
    "route_life = qa.route_life(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_change = {}\n",
    "for route in route_life:\n",
    "    if route_life[route] in route_change:\n",
    "        route_change[route_life[route]] = route_change[route_life[route]] + 1\n",
    "    else:\n",
    "        route_change[route_life[route]] = 1\n",
    "\n",
    "route_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.scatter([x for x in route_change.keys()], [y for y in route_change.values()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.get_route_changes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(year=2020, month=7, day=7, hour=8).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "end = datetime(year=2020, month=7, day=7, hour=9).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "ps_trace = esq.ps_trace_scan(es, start, end)\n",
    "ps_gen = esq.scan_gen(ps_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(str(PROJECT_ROOT / 'data' / 'route_changes.pa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Life'] = (7 * 24 / df['changetimes'].str.len())\n",
    "df = df.rename(columns={'changetimes':'Paths'})\n",
    "dfl = df.drop(['dest', 'sha', 'src'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = dfl.groupby(['Life']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = dfl.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "dfl.plot(x = 'Life', y = 'Paths', logy = True, kind = 'scatter', ax = ax)\n",
    "# dfl.plot(x = 'Life', y = 'Paths', kind = 'scatter', logx = True, logy = True, ax = ax[1])\n",
    "# ax[0].set_xlim(170, 0)\n",
    "ax.set_xlabel('Life of a path in hours')\n",
    "ax.set_ylabel('Paths count (' + str(dfl['Paths'].sum()) + ' total) - log scale')\n",
    "ax.set_title('Over 7 days (168 hrs), how long does any path between PS nodes stays stable?')\n",
    "# ax[1].set_xlim(170, 0)\n",
    "# ax[1].set_xlabel('Life of a path in hours (log scale)')\n",
    "# ax[1].set_ylabel('Paths count (log scale)')\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = dfl.copy()\n",
    "dfn['Paths'] = dfn['Paths'] / dfn['Paths'].sum()\n",
    "# dfn['Life'] = dfn['Life'] / dfn['Life'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "dfn.plot(x = 'Life', y = 'Paths', kind = 'scatter', ax = ax[0])\n",
    "dfn.plot(x = 'Life', y = 'Paths', kind = 'scatter', logx = True, logy = True, ax = ax[1])\n",
    "ax[0].set_xlabel('Life of a path (hours)')\n",
    "ax[0].set_ylabel('Paths (count)')\n",
    "ax[0].set_title('Over 7 days (168 hrs), how long does any path between PS nodes stays stable?')\n",
    "ax[1].set_xlabel('Life of a path (hours) - log scale')\n",
    "ax[1].set_ylabel('Paths (count) - log scale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain(*iters):\n",
    "    for iterable in iters:\n",
    "        yield from iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hops = []\n",
    "for i in range(7):\n",
    "    pst = pd.read_parquet(str(PROJECT_ROOT / 'data' / 'ps_trace' + str(i) + '.pa'))\n",
    "    hops = pst['hops'].tolist()\n",
    "    hops = pd.core.common.flatten(hops)\n",
    "    all_hops = chain(all_hops, hops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hops_df = pd.DataFrame(list(all_hops), columns=['Hops'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_counts = hops_df['Hops'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hop_c2 = pd.DataFrame(hop_counts).reset_index().rename(columns={'index':'Node', 'Hops': 'Count'})\n",
    "hop_c2.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_count():\n",
    "    with open('test.csv', 'w', newline='') as csvfile:\n",
    "        fieldnames = ['src', 'dest', 'route-sha1', 'hops', 'timestamp']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writerow({'src':'src', 'dest':'dest', 'route-sha1':'route-sha1', 'hops':'hops', 'timestamp':'timestamp'})\n",
    "        \n",
    "        start = datetime(year=2020, month=7, day=7, hour=8).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "        end = datetime(year=2020, month=7, day=7, hour=9).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "        ps_trace = esq.ps_trace_scan(es, start, end)\n",
    "        records = 0\n",
    "    #     df = pd.DataFrame(data=next(ps_trace)[\"_source\"])\n",
    "        for trace in esq.scan_gen(ps_trace):\n",
    "    #         df.append(trace['_source'], ignore_index=True)\n",
    "            writer.writerow(trace)\n",
    "            records += 1\n",
    "            if not records % 1000:\n",
    "                print(records)\n",
    "        print(records)\n",
    "    #     return df\n",
    "    \n",
    "print(timeit.timeit('df = record_count()', setup=\"from __main__ import record_count\", number=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_count():\n",
    "    data = []\n",
    "    start = datetime(year=2020, month=7, day=7, hour=8).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "    end = datetime(year=2020, month=7, day=7, hour=9).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "    ps_trace = esq.ps_trace_scan(es, start, end)\n",
    "    records = 0\n",
    "#     df = pd.DataFrame(data=next(ps_trace)[\"_source\"])\n",
    "    for trace in esq.scan_gen(ps_trace):\n",
    "#         df.append(trace['_source'], ignore_index=True)\n",
    "        data.append(trace)\n",
    "        records += 1\n",
    "        if not records % 1000:\n",
    "            print(records)\n",
    "    print(records)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_parquet(str(PROJECT_ROOT / 'data' / 'test.pa'), engine='pyarrow')\n",
    "#     return df\n",
    "    \n",
    "print(timeit.timeit('df = record_count()', setup=\"from __main__ import record_count\", number=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('test.csv')\n",
    "df = pd.read_parquet(str(PROJECT_ROOT / 'data' / 'test.pa'), engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(year=2020, month=7, day=7, hour=8).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "end = datetime(year=2020, month=7, day=14, hour=8).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "ps_trace = esq.ps_trace_scan(es, start, end, include=[\"hops\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a larger dataset\n",
    "\n",
    "Creating a generator spanning a day of data, I can identify routes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(year=2020, month=7, day=8, hour=8).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "end = datetime(year=2020, month=7, day=9, hour=8).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "ps_trace = esq.ps_trace_scan(es, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed = {}\n",
    "removed_edges = {}\n",
    "added_edges = {}\n",
    "\n",
    "for trace in ps_trace:\n",
    "    trace = trace['_source']\n",
    "    if not trace['src_production'] or not trace['dest_production']:\n",
    "        continue\n",
    "    try:\n",
    "        src = trace['src']\n",
    "        dest = trace['dest']\n",
    "        sha = trace['route-sha1']\n",
    "        hops = trace['hops']\n",
    "        max_rtt = trace['max_rtt']\n",
    "        looping = trace['looping']\n",
    "        time = trace['timestamp']\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    if (src, dest) in routes and sha != routes[(src, dest)]['sha'] and max_rtt > routes[(src, dest)]['max_rtt']:\n",
    "        og_path_edges = []\n",
    "        new_path_edges = []\n",
    "        removed = []\n",
    "        added = []\n",
    "\n",
    "        for i in range(len(routes[(src, dest)]['hops']) - 1):\n",
    "            s = routes[(src, dest)]['hops'][i]\n",
    "            d = routes[(src, dest)]['hops'][i+1]\n",
    "            og_path_edges.append((s, d))\n",
    "            og_path_edges.append((d, s))\n",
    "        \n",
    "        for i in range(len(hops) - 1):\n",
    "            s = hops[i]\n",
    "            d = hops[i+1]\n",
    "            new_path_edges.append((s, d))\n",
    "            new_path_edges.append((d, s))\n",
    "\n",
    "        for edge in og_path_edges:\n",
    "            if edge not in new_path_edges:\n",
    "                removed.append(edge)\n",
    "                if edge in removed_edges:\n",
    "                    removed_edges[edge]['count'] = removed_edges[edge]['count'] + 1\n",
    "                    removed_edges[edge]['time'].append(time)\n",
    "                else:\n",
    "                    removed_edges[edge] = {'count': 1, 'time': [time]}\n",
    "        \n",
    "        for edge in new_path_edges:\n",
    "            if edge not in og_path_edges:\n",
    "                added.append(edge)\n",
    "                if edge in added_edges:\n",
    "                    added_edges[edge]['count'] = added_edges[edge]['count'] + 1\n",
    "                    added_edges[edge]['time'].append(time)\n",
    "                else:\n",
    "                    added_edges[edge] = {'count': 1, 'time': [time]}\n",
    "        \n",
    "        changed[(src, dest)] = {'added': added, 'removed': removed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traceroute_changes(edges_impacted):\n",
    "    fig, ax = plt.subplots(figsize=(15, 7))\n",
    "    ax.set_title('Edges removed from formerly stable paths')\n",
    "    ax.set_ylabel('Number of paths an edge was formerly on')\n",
    "    ax.set_xlabel('Edges formerly on paths')\n",
    "    ax.bar([str(edge) for edge in edges_impacted.keys()], [edges_impacted[edge]['count'] for edge in edges_impacted.keys()])\n",
    "    ax.tick_params(labelrotation=20)\n",
    "#     ax.set_yticks([i for i in range(int(max(edges_impacted.values()))+1)])\n",
    "\n",
    "#     file_name = 'Traceroute-' + str(edge_removed[0]) + '_' + str(edge_removed[1]) + '.png'\n",
    "#     plt.savefig(str(PROJECT_ROOT / 'reports' / 'figures' / file_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traceroute_changes(removed_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
